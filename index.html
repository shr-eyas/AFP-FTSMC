<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <meta property="og:title" content="Adaptive Fuzzy Predictor based Fast Terminal Sliding Mode Controller Design for Two-Link Robot Manipulator"/>
  <meta property="og:url" content="https://shr-eyas.github.io/AFP-FTSMC/"/>
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>

  <title>Adaptive Fuzzy Predictor based Fast Terminal Sliding Mode Controller Design for Two-Link Robot Manipulator</title>
  
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">
  <link rel="icon" href="static/figures/icon2.png">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<body>

  
<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://shr-eyas.github.io">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>
    </div>

  </div>
</nav>

<section class="publication-header">
  <div class="hero-body">
    <div class="container is-max-widescreen">
      <!-- <div class="columns is-centered"> -->
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Adaptive Fuzzy Predictor based Fast Terminal Sliding Mode Controller Design for Two-Link Robot Manipulator</h1>
          <div class="is-size-3 publication-authors">
            ICC, 2024
          </div>
        </div>
    </div>
  </div>

</section>

<section class="publication-author-block">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <div class="is-size-5 publication-authors">
           <span class="author-block"><a href="https://shr-eyas.github.io" target="_blank">Shreyas Kumar</a>,</span>
           <span class="author-block"><a href="https://www.linkedin.com/in/bishesha-dash/" target="_blank">Bishesha Dash </a>,</span>
           <span class="author-block"><a href="https://www.iiests.ac.in/IIEST/Faculty/ee-roshni" target="_blank">Roshni Maiti</a>,</span>
           <span class="author-block"><a href="https://mnnit.ac.in/profile/dipayan" target="_blank">Dipayan Guha</a>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">IIEST, NIT Allahabad</span> 
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://ieeexplore.ieee.org/document/10883745" target="_blank"
                  class="external-link button is-normal is-rounded">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>

              
              <!-- PDF Link. -->
<!--              <span class="link-block">-->
<!--                <a href="static/source/ADDHEREPDF.pdf" target="_blank"-->
<!--                  class="external-link button is-normal is-rounded">-->
<!--                  <span class="icon">-->
<!--                    <i class="fas fa-file-pdf"></i>-->
<!--                  </span>-->
<!--                  <span>Paper</span>-->
<!--                </a>-->
<!--              </span>-->
<!--                            </span>-->
              <!-- </span> -->
              <!-- Colab Link. -->
<!--              <span class="link-block">-->
<!--                <a href="ADD HERE THE CODE" target="_blank"-->
<!--                class="external-link button is-normal is-rounded">-->
<!--                <span class="icon">-->
<!--                  <i class="fab fa-github"></i>-->
<!--                </span>-->
<!--                <span>Code</span>-->
<!--              </a>-->
<!--             </span>-->

<!--              <span class="link-block">-->
<!--                <a href="ADD HERE REPLICATE IF NEEDED" target="_blank"-->
<!--                class="external-link button is-normal is-rounded">-->
<!--                <span class="icon">-->
<!--                  <i class="fas fa-rocket"></i>-->
<!--                </span>-->
<!--                <span>Demo</span>-->
<!--              </a>-->
<!--              </span>-->
              <!-- </span> -->
              <!-- Colab Link. -->
            </div>
          </div>

        </div>
      </div>
    </div>
  </div>
</section>

<!-- 
<section class="hero is-small">
  <!~~ <div class="hero-body"> ~~>
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!~~ <div id="results-carousel" class="carousel results-carousel"> ~~>
      <div class="container">
      <div class="item">
      <div class="column is-centered has-text-centered">
        <img src="static/figures/teaser.png" alt="UNIMASKM"/>
      </div>

    </div>
  </div>
 <!~~  </div> ~~>
  </div>
  </div>
 <!~~  </div> ~~>
</section>
 -->

  <section class="hero is-small">
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="container">
        <div class="item">
          <p style="margin-bottom: 30px">
 
        <video poster="" id="tree" autoplay controls muted loop height="100%">
          <source src="static/figures/videos/overview.mp4"
          type="video/mp4">
        </video>
        </p>
        </div>
    </div>
  </div>
  </div>
  </div>
</section>
    
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">

        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            This article introduces a framework for complex human-robot collaboration tasks, such as the co-manufacturing of furniture. For these tasks, it is essential to encode tasks from human demonstration and reproduce these skills in a compliant and safe manner. Therefore, two key components are addressed in this work: motion generation and shared autonomy. We propose a motion generator based on a time-invariant potential field, capable of encoding wrench profiles, complex and closed-loop trajectories, and additionally incorporates obstacle avoidance. Additionally, the paper addresses shared autonomy (SA) which enables synergetic collaboration between human operators and robots by dynamically allocating authority. Variable impedance control (VIC) and force control are employed, where impedance and wrench are adapted based on the human-robot autonomy factor derived from interaction forces. System passivity is ensured by an energy-tank based task passivation strategy. The framework's efficacy is validated through simulations and an experimental study employing a Franka Emika Research 3 robot.
           </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<section class="hero is-small">
  <div class="hero-body">
    <div class="container">

      <div class="column is-centered has-text-centered">
        <img src="static/figures/overview.jpg" alt="Motivation of our model"/>
      </div>    
  </div>
</div>
</div>
</section>




<section class="section hero is-light">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
         <h2 class="title is-3">How does it work?</h2> 
        <div class="content has-text-justified">
          <p>
            Our framework leverages a sophisticated shared autonomy framework designed to enhance human-machine collaboration. At its core, it features a state-of-the-art motion generator capable of encoding task-specific movements and wrench profiles. This motion generator is adept at navigating complex, closed-loop paths while seamlessly incorporating obstacle avoidance, ensuring smooth and efficient operations across diverse environments.
          </p> <p>
          Building upon this foundation, our approach introduces a Variable Impedance and Force Control (VIC) system. This system is meticulously crafted to ensure precise trajectory following and force application, adjusting dynamically to different levels of human interaction and responding effectively to any human-induced changes. Such adaptability ensures that machines can work in perfect harmony with their human counterparts, enhancing productivity and safety.
        </p> <p>
        A key element of the system is the integration of an energy-tank-based passivation strategy. This strategy ensures the system remains stable and responsive even as it adjusts to changing control parameters and impedance levels, drawing on advanced concepts to maintain operational integrity.
      </p> <p>
        Proposed methodology has been rigorously tested through extensive experiments and simulations, demonstrating its effectiveness and reliability. This framework opens new avenues for human-machine collaboration, promising significant advancements in industrial, medical, and service applications.
        </p>
        <div class="columns is-centered has-text-centered">
            <div class="item">
          <p style="margin-bottom: 30px">
<!--
        <video poster="" id="tree" autoplay controls muted loop height="100%">
          <source src="static/figures/MDM-page.mp4"
          type="video/mp4">
        </video>
 --><br><br>
        </p>
        </div>
            </div>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

    
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">

      <div class="column is-centered has-text-centered">
        <img src="static/figures/modeloverview.png" alt=""/>
      </div>    
  </div>
</div>
</div>
</section>


<section class="section hero is-light">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
         <h2 class="title is-3">Comparison with SEDS and LPV-DS</h2> 
        <div class="content has-text-justified">
          <p>
            To evaluate the effectiveness of our method, we compared it with SEDS and LPV-DS by using the leaf shape from the LASA handwriting dataset, as shown in the above figure. Although both SEDS and LPV-DS managed to reach the final point, they couldn't accurately copy the original example. LPV-DS had a smaller error in following the path than SEDS, but still couldn't match the original example closely.
          </p>
          <p>
            In the above figure, the red lines show the original examples, black lines with arrows represent the path of movement, and solid black lines are the attempts to recreate the examples. Our method uses the average path from the examples, marked with a green dashed line in above figure. It's worth noting that our method only needs one example, chosen from many based on the best performance. Our findings show that points starting away from the example path eventually meet and then follow this average path, reaching the end goal. This proves our method can capture complex shapes, including those that don't directly move closer to the target over time. Our technique also works well with other shapes in the LASA handwriting dataset.
          </p>
        <div class="columns is-centered has-text-centered">
            <div class="item">
          <p style="margin-bottom: 30px">
<!--
        <video poster="" id="tree" autoplay controls muted loop height="100%">
          <source src="static/figures/MDM-page.mp4"
          type="video/mp4">
        </video>
 --><br><br>
        </p>
        </div>
            </div>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>


    
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div class="column is-centered has-text-centered">
        <img src="static/figures/exp.png" alt="dd"/>
      </div>    
  </div>
</div>
</div>
</section>


<section class="section hero is-light">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
         <h2 class="title is-3">Experiments</h2> 
        <div class="content has-text-justified">
            <p>
            In our research, we delved into an innovative approach enabling robots to acquire and replicate precise movements and exert the necessary force for task execution, utilizing a hands-on teaching strategy known as kinesthetic teaching. This technique proves invaluable for tasks requiring direct environmental interaction. A standout feature of our methodology is the seamless transition of control from human to robot during tasks, facilitating an efficient and intuitive collaboration. We evaluated our method through a basic yet illustrative test: button pressing and polishing.
            </p> <p>
            The test involved a person guiding the robot's arm to different positions to teach the task. Upon release, the robot autonomously executed the task, demonstrating its capability to accurately follow the instructed path and press the button, with the arm movements captured in 3D. The robot exhibited adaptive behavior when manually adjusted, becoming more compliant for smoother interaction, thereby enhancing the collaborative experience.
            </p> <p>
            Building on previous experiments where human-applied force directed robot control, we explored complex task execution, such as polishing intricately shaped objects. Utilizing a leaf-shaped object from the LASA handwriting dataset, the robot learned and replicated the polishing motion. We monitored the robot's arm during learning, task execution, and obstacle avoidance, emphasizing its ability to maintain a close trajectory with minimal deviation and apply appropriate force for polishing.
            </p> <p>
            The robot demonstrated intelligent obstacle navigation, resuming its task without disruption. An integral experiment component was the introduction of an "energy tank" concept to regulate energy consumption, ensuring safe, efficient operation. The robot's energy management system automatically activated a low-energy mode upon reaching critical levels, halting tasks to prevent issues.
            </p> <p>
            This study showcased the robot's real-time adaptability in movement and force application, managing complex tasks and obstacles while effectively regulating its energy consumption for safe, efficient functionality.
            </p> 
            
        <div class="columns is-centered has-text-centered">
            <div class="item">
          <p style="margin-bottom: 30px">
<!--
        <video poster="" id="tree" autoplay controls muted loop height="100%">
          <source src="static/figures/MDM-page.mp4"
          type="video/mp4">
        </video>
 --><br><br>
        </p>
        </div>
            </div>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

 


    
<section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
@article{jadav2024salads,
  title={Shared Autonomy via Variable Impedance Control and Virtual Potential Fields for Encoding Human Demonstrations},
  author={Jadav, Shail and Heidersberger, Johannes and Ott, Christian and Lee,Dongheui},
  journal={2024 IEEE International Conference on Robotics and Automation (ICRA)},
  year={2024}
}</code></pre>
    </div>
</section>




<footer class="footer">
 <!--  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
      href="https://homes.cs.washington.edu/~kpar/nerfies/videos/nerfies_paper.pdf">
      <i class="fas fa-file-pdf"></i>
    </a>
    <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
      <i class="fab fa-github"></i>
    </a>
  </div> -->
  <div class="columns is-centered">
    <div class="column is-8">
      <div class="content">
        <p>
          This website is licensed under a <a rel="license"
          href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
        Commons Attribution-ShareAlike 4.0 International License</a>.
      </p>
      <p>
        Website source code based on the <a href="https://nerfies.github.io/"> Nerfies</a> project page. If you want to reuse their <a
        href="https://github.com/nerfies/nerfies.github.io">source code</a>, please credit them appropriately.
      </p>
    </div>
  </div>
</div>
</div>
</footer>

  </body>
  </html>